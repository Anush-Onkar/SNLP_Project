{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f12ad8",
   "metadata": {},
   "source": [
    "# SNLP 2021 Final Project \n",
    "\n",
    "Submitted by\n",
    "\n",
    "Name 1:Anush Onkarappa\n",
    "\n",
    "Student id 1:7010620\n",
    "\n",
    "Email 1:anon00001@stud.uni-saarland.de\n",
    "\n",
    "Name 2:Rahul Mudambi Venkatesh\n",
    "\n",
    "Student id 2:7015710\n",
    "\n",
    "Email 2:ramu00001@stud.uni-saarland.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f9ba3",
   "metadata": {},
   "source": [
    "## **1 Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968e1f2",
   "metadata": {},
   "source": [
    "We have used corpora `data/bengali_corpus.txt` and `data/alice_in_wonderland.txt`.\n",
    "\n",
    "### Command to run preproccessing and splitting of corpus into train & test(20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278fd8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For English corpus\n",
    "!python preprocess_english.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52443211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Bengali corpus\n",
    "!python preprocess_bengali.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fb02f",
   "metadata": {},
   "source": [
    "## **2 Subword Segmentation**\n",
    "\n",
    "### Command to generate subwords for both English and Bengali\n",
    "Syntax for the command- python subwords.py **langauge** **vocabsize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d07176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For English corpus\n",
    "!python subwords.py english 49  #here 49 is the vocabsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Bengali corpus\n",
    "!python subwords.py english 32  #here 32 is the vocabsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d392f",
   "metadata": {},
   "source": [
    "Resulting files: `en_s1.txt`, `en_s2.txt`, `en_s3.txt`, `bn_s1.txt`, `bn_s2.txt` and `bn_s3.txt` can be found in the current directory \n",
    "\n",
    "Trails for the above task to find the best performing vocabulary size for `s2` and `s3` can be found in folder `rnnlms/temp_models`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e65a65",
   "metadata": {},
   "source": [
    "### Observation on Word Segmentation\n",
    "\n",
    "It can be seen that the generated subword files have a new character \"_\". This indicates the beginning of words, which is necessary for Bype-Pair Encoding. For s1, all the characters are separated, which indicates character-level tokenization. For s2, few characters were grouped, while for s3, more characters were grouped and the tokens were close to words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e326e",
   "metadata": {},
   "source": [
    "## **3 LM Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311186bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to move to the training folder\n",
    "%cd rnnlms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe54e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example command to train LM \n",
    "!/home/snlp-project-21/rnnlm/rnnlm \\\n",
    "    -train ../subwords/english/32.txt \\\n",
    "    -valid ../subwords/english/32_test.txt \\\n",
    "    -rnnlm model \\\n",
    "      -hidden 40 \\\n",
    "      -rand-seed 1 \\\n",
    "      -debug 2 \\\n",
    "      -bptt 3 \\\n",
    "      -class 32\n",
    "\"\"\"\n",
    "The above command is run for the baseline first. The model and output is moved to:\n",
    "s1_bengali/bengali_49 - for subwords s1, bengali\n",
    "s2_bengali/bengali_100 - for subwords s2, bengali\n",
    "s3_bengali/bengali_1700 - for subwords s3, bengali\n",
    "s1_english/english_32 - for subwords s1, english\n",
    "s2_english/english_100 - for subwords s2, english\n",
    "s3_english/english_2600 - for subwords s3, english\n",
    "\n",
    "Then, hyperparameter tuning is done, which is discussed in the summary.\n",
    "The models and outputs for the tuning are moved to:\n",
    "s1_bengali/Trials - for subwords s1, bengali\n",
    "s2_bengali/Trials - for subwords s2, bengali\n",
    "s3_bengali/Trials - for subwords s3, bengali\n",
    "s1_english/Trials - for subwords s1, english\n",
    "s2_english/Trials - for subwords s2, english\n",
    "s3_english/Trials - for subwords s3, english\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261f1c0",
   "metadata": {},
   "source": [
    "## **4 Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to move to text generation folder\n",
    "%cd Text_Gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42141e5",
   "metadata": {},
   "source": [
    "The train and test files should be copied to: \n",
    "    \n",
    "    data/english for english\n",
    "    data/bengali for bengali\n",
    "\n",
    "and renamed suitably(this is already done)\n",
    "\n",
    "The best perfoming model should be copied to : \n",
    "    \n",
    "    models/english for english\n",
    "    models/bengali for bengali\n",
    "\n",
    "and renamed suitably(this is already done)\n",
    "\n",
    "The corresponding sentence speech model should be copied to : \n",
    "    \n",
    "    SP_models/english for english\n",
    "    SP_models/bengali for bengali\n",
    "\n",
    "and renamed suitably(this is already done)\n",
    "\n",
    "Now we can run the text generation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd script\n",
    "!./test_english_s1.sh\n",
    "!./test_english_s2.sh\n",
    "!./test_english_s3.sh\n",
    "!./test_bengali_s1.sh\n",
    "!./test_bengali_s2.sh\n",
    "!./test_bengali_s3.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a219fbd",
   "metadata": {},
   "source": [
    "The generated text files are stored in temp folder which are moved to \n",
    "\n",
    "    s1_bengali/ for subwords s1, bengali\n",
    "    s2_bengali/ for subwords s2, bengali\n",
    "    s3_bengali/ for subwords s3, bengali\n",
    "    s1_english/ for subwords s1, english\n",
    "    s2_english/ for subwords s2, english\n",
    "    s3_english/ for subwords s3, english\n",
    "\n",
    "Now we have to decode the generated text files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61542884",
   "metadata": {},
   "source": [
    "Syntax for the command-python decode.py **inputdirectory** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example command \n",
    "!python decode.py s1_english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9ea90a",
   "metadata": {},
   "source": [
    "### Observation of generated texts of size 100 for both languages\n",
    "\n",
    "Examining s1_100.txt for all the models, we can observe that quality and understandability of the generated text for english increased from s1 to s3. This is because the subwords in s3 are closer to words. Thus the probability of more correct words being generated is high for this case. The quality is least for s1, as it is on a character-level. Thus, during generation combinations of characters which have no meaning can occur frequently. The observation is similar to bengali as we could find more complete words for s3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdf673a",
   "metadata": {},
   "source": [
    "## **5 OOV comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to move to the OOV-Comparison folder\n",
    "%cd ..\n",
    "%cd OOV_Comparison/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will print the OVV rate of english and bengali respectively for train data without any Augmentation\n",
    "!python task5.py english no #no is passed here to indicate no Augmentation of the data\n",
    "!python task5.py bengali no #no is passed here to indicate no Augmentation of the data\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "#It also plots the OVV rates against k, here k=0 indicates no Augmentation \n",
    "!python task5.py english yes #yes is passed here to indicate Augmentation of the data with the previously generated data\n",
    "!python task5.py bengali yes #yes is passed here to indicate Augmentation of the data with the previously generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c64eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from augment_vocab import augment\n",
    "\n",
    "# Graph Plotting\n",
    "for language in [\"english\", \"bengali\"]:\n",
    "    s1 = [augment(language, \"\")]\n",
    "    s2 = [augment(language, \"\")]\n",
    "    s3 = [augment(language, \"\")]\n",
    "\n",
    "    k_vals = [10,100,1000,10000,100000,1000000,10000000]\n",
    "\n",
    "    for k in k_vals:\n",
    "        s1.append(augment(language, \"../Text_Gen/s1_{}/s1_{}.txt\".format(language,k)))\n",
    "        s2.append(augment(language, \"../Text_Gen/s2_{}/s2_{}.txt\".format(language,k)))\n",
    "        s3.append(augment(language, \"../Text_Gen/s3_{}/s3_{}.txt\".format(language,k)))\n",
    "\n",
    "\n",
    "    #-----------\n",
    "    f1 = plt.figure()\n",
    "    ax1 = f1.add_subplot(111)\n",
    "    ax1.set_xlabel(\"k\")\n",
    "    ax1.set_ylabel(\"OOV Rate\")\n",
    "    ax1.set_title(\"OOV rates for subwords for {} (non-log scale)\".format(language))\n",
    "\n",
    "    y_vals = s1\n",
    "    x_vals = [0] + k_vals\n",
    "    ax1.plot(x_vals,y_vals,\"-..\")\n",
    "\n",
    "    y_vals = s2\n",
    "    x_vals = [0] + k_vals\n",
    "    ax1.plot(x_vals,y_vals,\"-..\")\n",
    "\n",
    "    y_vals = s3\n",
    "    x_vals = [0] + k_vals\n",
    "    ax1.plot(x_vals,y_vals,\"-..\")\n",
    "\n",
    "    plt.legend([\"s1\",\"s2\",\"s3\"])\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    #-----------\n",
    "\n",
    "\n",
    "    #-----------\n",
    "    f1 = plt.figure()\n",
    "    ax1 = f1.add_subplot(111)\n",
    "    ax1.set_xlabel(\"k\")\n",
    "    ax1.set_ylabel(\"OOV Rate\")\n",
    "    ax1.set_title(\"OOV rates for subwords for {} (log scale)\".format(language))\n",
    "\n",
    "    y_vals = s1\n",
    "    x_vals = [0] + k_vals\n",
    "    ax1.loglog(x_vals,y_vals,\"-..\")\n",
    "\n",
    "    y_vals = s2\n",
    "    x_vals = [0] + k_vals\n",
    "    ax1.loglog(x_vals,y_vals,\"-..\")\n",
    "\n",
    "    y_vals = s3\n",
    "    x_vals = [0] + k_vals\n",
    "    ax1.loglog(x_vals,y_vals,\"-..\")\n",
    "\n",
    "    plt.legend([\"s1\",\"s2\",\"s3\"])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e3b52",
   "metadata": {},
   "source": [
    "### Observation from the plots:\n",
    "\n",
    "* With increase in k(size of the generated corpus) OOV rates  for the 3 models decreases\n",
    "* It can be seen that the OOV rates are the least for model augmented with subwords closer to words(s3)\n",
    "* For English language, the difference between subwords s2 and s3 is smaller than that for Bengali\n",
    "* For any practical application we can use s3 as OOV rate is the least.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ea9c41",
   "metadata": {},
   "source": [
    "## **6. Analysis**\n",
    "\n",
    "The aim of this project is to overcome/alleviate the problem of Out Of Vocabulary(OOV) words, while building a language model. The approach used in this project is as follows:\n",
    "\n",
    "* First, train a RNN language model with subword tokens generated from the corpora. Tune the hyperparameters of the model\n",
    "* Use the RNN model trained to synthesize artificial text.\n",
    "* Augment the train data of the corpus with the generated text and calculate the OOV rates.\n",
    "\n",
    "The two natural laguages used in this project are English and Bengali.\n",
    "\n",
    "The project was divided into five tasks which are discussed as follows:\n",
    "\n",
    "* **Data Preperation** In this step, the respective corpora for English and Bengali were preprocessed and divided into train and test files, with test files consisting of 20% of the sentences of corpus of each language.\n",
    "\n",
    "\n",
    "* **Subword Segmentation** In this step, the SentencePiece text tokenizer was used for different vocabulary sizes, which implements Byte-Pair Encoding, on the sentences in the train files for each language. Three types of subword segmentation was performed in this case for each language: charater-level(s1), subwords close to characters(s2), and subwords close to words(s3). On experimenting with various vocabulary sizes (this can be found in `subwords/english` and `subwords/bengali`, and in `rnnlms/temp_models` directories), the optimal vocabulary sizes were found to be as follows:\n",
    "\n",
    "    * For s1, for english, the vocabulary size was 32\n",
    "    * For s2, for english, the vocabulary size was 100\n",
    "    * For s3, for english, the vocabulary size was 2600\n",
    "     \n",
    "    * For s1, for bengali, the vocabulary size was 49\n",
    "    * For s2, for bengali, the vocabulary size was 100\n",
    "    * For s3, for bengali, the vocabulary size was 1700\n",
    "    \n",
    "    \n",
    "* **LM Training** In this step, RNNLMs were trained on s1, s2 and s3 for both languages and the hyperparameters were tuned to optimize Perplexities/OOV Rates. A brief description of the hyperparameter is given below:\n",
    "\n",
    "    * For s1, English baseline vocab size was found to be 32 which gave optimum Perplexity of 5.3491. \n",
    "    To further reduce the Perplexity value Hyper parameter tuning was done on parameters BPTT and Hidden layers. For BPTT=4 & H=80 best performance Perplexity value 4.666 was obtained.It was also observed that with decrease in BPTT value, Perplexity value increased and with Hidden layer value >80 PL value decreased negligibly.  \n",
    "    All the trails carried out can be found in the `Trails` folder of `s1_english`.\n",
    "\n",
    "    * For s2, English baseline vocab size was found to be 100 which gave optimum Perplexity of 16.4355. \n",
    "    To further reduce the Perplexity value Hyper parameter tuning was done on parameters BPTT and Hidden layers. For BPTT=1 & H=80 best performance Perplexity value 14.232 was obtained. It was also observed that with decrease in BPTT value, Perplexity value decreased and with Hidden layer value >80 PL value decreased negligibly.  \n",
    "    All the trails carried out can be found in the `Trails` folder of `s2_english`.\n",
    "\n",
    "    * For s3, English baseline vocab size was found to be 2600 which gave optimum Perplexity of 195.5279. \n",
    "    To further reduce the Perplexity value Hyper parameter tuning was done on parameters BPTT and Hidden layers. For BPTT=4 & H=40 best performance Perplexity value 195.0037 was obtained. It was also observed that with decrease in BPTT value and increase in Hidden layer value, Perplexity value increased.\n",
    "    All the trials carried out can be found in the `Trails` folder of `s3_english`.\n",
    "\n",
    "    * For s1, Bengali baseline vocab size was found to be 49 which gave optimum Perplexity of 7.7006. \n",
    "    To further reduce the Perplexity value Hyper parameter tuning was done on parameters BPTT and Hidden layers. For BPTT=4 & H=130 best performance Perplexity value 5.839 was obtained.It was also observed that with decrease in BPTT value, Perplexity value increased and with Hidden layer value >130 PL value decreased negligibly. \n",
    "    All the trails carried out can be found in the `Trails`folder of `s1_bengali`.\n",
    "\n",
    "    * For s2, Bengali baseline vocab size was found to be 100 which gave optimum Perplexity of 17.9806. \n",
    "    To further reduce the Perplexity value Hyper parameter tuning was done on parameters BPTT and Hidden layers. For BPTT=3 & H=100 best performance Perplexity value 13.623 was obtained.It was also observed that with decrease in BPTT value, Perplexity value increased and with Hidden layer value >100 PL value decreased negligibly. \n",
    "    All the trails carried out can be found in the `Trails` folder of `s2_bengali`.\n",
    "\n",
    "    * For s3, Bengali baseline vocab size was found to be 1700 which gave optimum Perplexity of 228.5574. \n",
    "    To further reduce the Perplexity value Hyper parameter tuning was done on parameters BPTT and Hidden layers. For BPTT=4 & H=50 best performance Perplexity value 228.2559 was obtained.It was also observed that with decrease in BPTT value, Perplexity value increased and with Hidden layer value >50 PL value increased.  \n",
    "    All the trails carried out can be found in the `Trails` folder of `s3_bengali`\n",
    "\n",
    "\n",
    "* **Text Generation** In this step, each of the RNNLM models trained (for each language, each of the subword-segmentation types) were used to synthesize artificial texts, with corpus sizes ranging from 10,100,...,10000000. It was observed that the quality of the texts generated increased from s1 to s3.\n",
    "\n",
    "\n",
    "* **OOV Comparison** In this step, the OOV rates were calculated by augemnting the train data with the generated data(of different sizes), for each of the models and we plotted for comparison. The OOV rates for all models (for both languages) decreased with increased size of the generated data augmented. Overall, the OOV rates were more for Bengali than English, as Bengali is morphologically richer than English and thus has more syntactic and semantic constructs.\n",
    "\n",
    "To conclude, handling OOV words is important while building a language model, as they result in vanishing MLEs. OOV rates can be reduced by augmenting artificial data to train data, which in this project, is achieved using RNNLMs. Possible improvements/future work include:\n",
    "\n",
    "* Experimenting with different subword tokenizers and comparing the results, to improve the tokenization (Eg. Bert Tokenizer)\n",
    "* Experiment with other Neural Language Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
